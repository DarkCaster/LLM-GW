# Implementing Phase 1 of application: Engine Management Foundation (Bottom Layer)

Here you need to implement `engine/` sub-package according to the plan below.

## Step 1: `engine/engine_client.py` - Base EngineClient Class

**Purpose:** Abstract base class for HTTP communication with LLM engines.

**Implementation Details:**

- Create abstract base class `EngineClient` with the following responsibilities:
  - Define abstract method `estimate_tokens(request_data: dict) -> int` that subclasses must implement to calculate token requirements from incoming requests.
  - Define abstract method `forward_request(session: aiohttp.ClientSession, url: str, request_data: dict) -> aiohttp.ClientResponse` that subclasses must implement to process request, this method may transform request before forwarding it to engine, and may also transform response back before returning.
  - Store logger instance for this class.
  - Define abstract method `check_health() -> bool`, checks engine health and return true if engine is up and running, false otherwise.

NOTE: `request_data` dict generated by caller from `await request.json()` where `request` is `web.Request` type

**Dependencies:** `aiohttp`, `asyncio`, `logging`, `abc` (for abstract base class)

**Why:** This is the foundation for all engine communication. Having the abstract interface defined first allows us to implement concrete engines and test them independently.

## Step 2: `engine/llamacpp_engine.py` - LlamaCppEngine Class

**Purpose:** Concrete implementation of EngineClient for llama.cpp engines.

**Implementation Details:**

- Create `LlamaCppEngine` class that inherits from `EngineClient`:
  - Implement `estimate_tokens(request_data: dict) -> int`:
    - For now just create a stub method that return any random value between 1 and 512, we will address this later.
  - Implement `forward_request` method:
    - Checks that request is for endpoint "/v1/chat/completions", throw exeption if not.
    - Call to `_transform_response` to modify request before forwarding.
    - Forward request to the `llama-server` endpoint.
    - Must properly handle normal and streaming responses.
  - Implement `_transform_response(response_data: dict) -> dict`:
    - For now just create a stub retunring original response, we will address this later.
  - Implement `check_health()`:
    - Use `/health` endpoint.
  - Store llama.cpp-specific service base url on start
  - Initialize class with all configuration parameters directly from constructor (for now its only base url).

**Dependencies:** `engine_client.py`, `aiohttp`, `asyncio`, `json`, `logging`

**Why:** This provides a concrete implementation we can test. We can verify the engine client pattern works before building the process management layer.

## Step 3: `engine/engine_process.py` - EngineProcess Class

**Purpose:** Wrapper for managing a single LLM engine subprocess.

**Implementation Details:**

- Create `EngineProcess` class with the following responsibilities:
  - Constructor accepts: `binary_path: str`, `args: List[str]`, `work_dir: str = None`
  - Implement `async start()` method:
    - Use `asyncio.subprocess.create_subprocess_exec()` to spawn the engine binary
    - Pass binary_path and args to the subprocess
    - Set `stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE` for log capture
    - Set working directory if provided
    - Store the subprocess handle
    - Create asyncio tasks to continuously read and log stdout/stderr
    - Store process start time
    - Set status to "running"
  - Implement `async stop(timeout: float = 10.0)` method:
    - Send SIGTERM to the process (graceful shutdown)
    - Wait for process to exit with timeout
    - If timeout expires, send SIGKILL (forceful shutdown)
    - Clean up stdout/stderr reader tasks
    - Set status to "stopped"
  - Implement `is_running() -> bool` property:
    - Check if subprocess is still alive
    - Return True if running, False otherwise
  - Implement `get_pid() -> Optional[int]` property:
    - Return process PID if running
    - Return None if not running
  - Implement `get_status() -> str` property:
    - Return status string: "running", "stopped", "crashed"
    - Detect crashed state if process exited unexpectedly
  - Implement async log reader methods:
    - `_read_stdout()` - continuously read stdout and log with INFO level
    - `_read_stderr()` - continuously read stderr and log with WARNING level
    - Both should run as background tasks while process is alive
  - Store logger instance for output logging

**Dependencies:** `asyncio`, `asyncio.subprocess`, `logging`, `typing`, `signal`

**Why:** Process management is the next layer. We can test spawning and controlling engine processes independently before integrating with the client layer.

## Step 4: `engine/engine_manager.py` - EngineManager Class

**Purpose:** Coordinate engine lifecycle - stop old engines, start new ones, track state.

**Implementation Details:**

- Create `EngineManager` class:
  - Implement `current_engine_client: EngineClient | None` property - client for current engine (if running)
  - Implement `check_model_configuration (model_name: str, required_config: dict) -> bool` - check that currenltly running model suitable for selected configuration, required_config dict contents may vary depending on engine type:
    - Return true if engine for requested model is already loaded and if `_check_llamacpp_config` for llama engine return true
  - Use `_lock: asyncio.Lock` - to prevent concurrent engine switches
  - Implement `_check_llamacpp_config(required_config: dict) -> bool`:
    - for now create stub that return true if currently loaded engine is a llamacpp type.
  - Implement `async ensure_engine(model_name: str, required_config: dict, engine_type: str) -> EngineClient`:
    - Acquire lock to prevent race conditions.
    - If no model with model_name present - throw exception (use `PyLuaHelper` cfg object to get index of model with needed model name).
    - Get engine type for requested model_name by index (use `PyLuaHelper` cfg object).
    - We can only support llamacpp engine type now, throw exception if needed model is other engine type.
    - The rest of the logic is for llamacpp engine only for now:
      - Compare requested config with current config by running `_check_llamacpp_config`:
      - If check succeed then verify health and return current engine client
    - If engine start/restart needed:
      - Call `await self._stop_current_engine()`
      - Call `await self._start_new_engine(model_name, required_config, engine_type)`
    - Return the engine client
  - Implement `async _stop_current_engine()`:
    - If no engine running, return immediately
    - Log the shutdown
    - Call `await current_engine_process.stop(timeout=15.0)`
    - Set current_engine_process and current_engine_client to None
    - Clear current config - like engine name, its model and its config, etc...
  - Implement `async _start_new_engine(model_name: str, required_config: dict, engine_type: str)`:
    - We can only support llamacpp engine type now, throw exception if engine_type is other tupe, the rest of the logic is for llamacpp engine only for now.
    - Extract binary, args, connect URL, use `PyLuaHelper` cfg object, get variant_index from required_config table
    - Create appropriate EngineClient based on engine_type:
      - If `engine_type == "llama.cpp"`, create `LlamaCppEngine(connect_url)` (the only option for now)
    - Create `EngineProcess(binary, args)`
    - Call `await engine_process.start()`
    - Call `await self._wait_for_engine_ready(engine_client, timeout=60.0)`
    - Store engine_process, engine_client, used required_config, model_name in instance variables
  - Implement `async _wait_for_engine_ready(engine_client: EngineClient, timeout: float) -> bool`:
    - Loop with sleep intervals (e.g., every 0.25 second)
    - Call `await engine_client.check_health()`
    - If health check passes, return True
    - If timeout expires, raise TimeoutError
    - Log progress (e.g., "Waiting for engine to be ready... X.YYY sec")
  - Implement `async shutdown()`:
    - Stop current engine if running
    - Clean up resources

**Dependencies:** `engine_process.py`, `engine_client.py`, `llamacpp_engine.py`, `asyncio`, `logging`, `typing`

**Why:** This ties together process management and client communication. We can now test the full engine lifecycle: start, health check, stop, restart with different config.

## Step 5: `engine/__init__.py` - Engine Package Interface

**Purpose:** Expose public API of the engine package.

**Implementation Details:**

- Import and expose:
  - `EngineManager` (main interface)
  - `EngineClient` (for type hints)
  - `LlamaCppEngine` (for instantiation)
- Define `__all__` list with exported classes

**Dependencies:** All engine package modules

**Why:** Package initialization to provide clean import interface.

---

# Implementing Phase 2: Model Selection Logic (Middle Layer)

Here you need to implement `models/` sub-package according to the plan below.

## Step 1: `models/model_selector.py` - ModelSelector Class

**Purpose:** Analyze requests and select appropriate model variant based on context requirements.

**Implementation Details:**

- Create `ModelSelector` class:
  - Constructor accepts `engine_manager: EngineManager` and `cfg: PyLuaHelper` (configuration object)
  - All model name and other parameter searches, should be performed dynamically via cfg object.
  - Implement `async select_variant(model_name: str, request_data: dict)`:
    - Get first model index with matching name from cfg object (model table)
    - If not found, raise `ValueError(f"Model '{model_name}' not found in configuration")`
    - Get engine type for this model, currently we can only support `llama.cpp` engine, fail on other engines.
    - Construct `required_config` dict with following values: `operation`=context_estimation,
    - Call `engine_manager.ensure_engine` with `required_config` dict to get engine-client for context size requirements estimation.
    - Call to `estimate_tokens` of engine-client object to get presise context size requirement
    - Iterate over model's variant indices, select first variant-index where context size is >= context size requirement
    - Construct `required_config` dict with following values: `operation`=text_query, `variant_index`=variant-index number, `context_size_required`=ctx size requirement number
    - Call `engine_manager.ensure_engine` with `required_config` dict to get engine-client for final operation
    - Return normally if final engine-client is not null, or if final engine-client is null - raise ValueError
  - Implement `list_models() -> List[str]`:
    - Return list of all configured model names, get it via iterating `cfg` object
    - Used for /v1/models endpoint

**Dependencies:** `python_lua_helper.PyLuaHelper`, `engine.EngineManager`, `typing`, `logging`

**Why:** This implements the intelligence of variant selection. It requires the engine layer to be complete (for tokenization), but is independent of the HTTP server layer.

NOTE: `request_data` dict produced from `await request.json()` where `request` is `web.Request` type

## Step 2: `models/__init__.py` - Models Package Interface

**Purpose:** Expose public API of the models package.

**Implementation Details:**

- Import and expose `ModelSelector`
- Define `__all__` list

**Dependencies:** `model_selector.py`

---

# Implementing Phase 3: HTTP Server Layer (Top Layer)

Here you need to implement `server/` sub-package according to the plan below.

## Step 1: `server/request_handler.py` - RequestHandler Class

**Purpose:** Process individual HTTP requests, coordinate model selection and engine management.

**Implementation Details:**

- Create `RequestHandler` class with the following responsibilities:
  - Constructor accepts:
    - `model_selector: ModelSelector`
    - `engine_manager: EngineManager`
    - `cfg: PyLuaHelper` (for any server-level config)
  - Implement `async handle_models_list(request: aiohttp.web.Request) -> aiohttp.web.Response`:
    - This is the handler for OpenAI API `/v1/models` endopoint
    - Call `model_selector.list_models()`
    - Build OpenAI-compatible response format:
      - `{"object": "list", "data": [{"id": model_name, "object": "model", ...}, ...]}`
      - Probably you should omit or fake `"meta": { ... }` object in response, because it is impossible to maintain because of application architecture
    - Return JSON response
  - Implement `async handle_request(request: aiohttp.web.Request) -> aiohttp.web.Response`:
    - This is a handler any other endopoints
    - Parse JSON body from request to `request_data` using `await request.json()`
    - Validate required fields (model only, for now)
    - Extract model name from request
    - Exrtact url suffix (endpoint) from request: trim protocol, domain and port, you should left with endpoint like `/v1/chat/completions`
    - Call `await model_selector.select_variant(model_name, request_data)`
    - Call `await engine_manager.current_engine_client.forward_request(url, request_data)` to send request to engine
    - Return response
    - In case of any exception in this method try returning appropriate HTTP status code
  - Use logger from utils for logging

**Dependencies:** `aiohttp`, `json`, `logging`, `models.ModelSelector`, `engine.EngineManager`

**Why:** This is the business logic layer that connects core components. It requires both model selection and engine management to be complete.

## Step 2: `server/gateway_server.py` - GatewayServer Class

**Purpose:** Main HTTP server that listens and routes requests.

**Implementation Details:**

- Create `GatewayServer` class with the following responsibilities:
  - Constructor accepts `request_handler: RequestHandler`, `cfg: PyLuaHelper`
  - Implement `async start()`:
    - Create `aiohttp.web.Application()`
    - Register routes:
      - `GET /v1/models` → `request_handler.handle_models_list`
      - `POST /v1/chat/completions` → `request_handler.handle_request`
    - Create list of runners for IPv4 and IPv6 from `cfg` with table `server`:
      - If `listen_v4 != "none"`, parse address/port and create IPv4 runner
      - If `listen_v6 != "none"`, parse address/port and create IPv6 runner
      - Throw exception if no runners created
    - Setup and start all runners
    - Store runners and sites for cleanup
  - Implement `async stop()`:
    - Stop all site runners and make needed cleanup
  - Implement `async run()`:
    - Call `await start()`
    - Wait forever (until interrupted)
    - Handle graceful shutdown with `async stop()` on interrupt
  - Use logger from utils for logging

**Dependencies:** `aiohttp`, `asyncio`, `logging`, `server.RequestHandler`, `models.ModelSelector`, `engine.EngineManager`, `config.ConfigLoader`

**Why:** This is the HTTP server entry point.

## Step 3: `server/__init__.py` - Server Package Interface

**Purpose:** Expose public API of the server package.

**Implementation Details:**

- Import and expose:
  - `GatewayServer` (main server class)
  - `RequestHandler` (for testing)
- Define `__all__` list

**Dependencies:** All server package modules

---

# Implementing Phase 4: Integration and Main Entry Point

## Step 1: Update `main.py` - Application Entry Point

**Purpose:** Wire everything together and provide command-line interface.

**Implementation Details:**

- Modify existing `main()` function:
  - After loading configuration, create `GatewayServer(cfg)`
  - Replace placeholder comment `# start server here and wait for interrupt` with:
    - `await server.run()`
  - Make `main()` async: `async def main():`
  - Add proper signal handling for graceful shutdown (SIGINT, SIGTERM)
  - In finally block:
    - Call `await server.stop()` before cleanup
  - Handle asyncio properly:
    - If script run directly, use `asyncio.run(main())`
- Add signal handlers:
  - Register handlers for SIGINT and SIGTERM
  - Set flag to trigger graceful shutdown
  - Server should detect flag and stop gracefully

**Dependencies:** `asyncio`, `signal`, `server.GatewayServer`

**Why:** Final integration step that creates a runnable application.
